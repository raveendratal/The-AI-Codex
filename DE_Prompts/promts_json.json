{
  "PySpark": {
    "pyspark_001": "Generate PySpark code to create a DataFrame with sample data for customers (id, name, age, city).",
    "pyspark_002": "Write PySpark code to read a CSV from a UC volume into a Delta table.",
    "pyspark_003": "Create a PySpark schema for nested JSON with arrays and structs.",
    "pyspark_004": "Convert multiple columns to correct datatypes using withColumn and cast.",
    "pyspark_005": "Perform multi-level aggregation using groupBy and agg.",
    "pyspark_006": "Filter rows using complex AND/OR boolean conditions.",
    "pyspark_007": "Flatten deeply nested struct columns.",
    "pyspark_008": "Write left, right, inner, outer, semi, and anti joins with PySpark.",
    "pyspark_009": "Optimize large joins using broadcast().",
    "pyspark_010": "Handle skew joins using salting.",
    "pyspark_011": "Use row_number(), rank(), dense_rank() with window partitions.",
    "pyspark_012": "Get first and last purchase per customer using window functions.",
    "pyspark_013": "Compute rolling averages over date windows.",
    "pyspark_014": "Extract year, month, day, week, quarter from timestamps.",
    "pyspark_015": "Clean text fields using regexp_replace.",
    "pyspark_016": "Calculate time differences in seconds/minutes/days.",
    "pyspark_017": "Write PySpark MERGE INTO statement to implement CDC.",
    "pyspark_018": "Perform Delta time travel queries using versionAsOf.",
    "pyspark_019": "Implement CDC merge logic in PySpark.",
    "pyspark_020": "Create a Structured Streaming Auto Loader pipeline with checkpointing.",
    "pyspark_021": "Generate PySpark code to stream into Bronze table.",
    "pyspark_022": "Apply watermarking to handle late data.",
    "pyspark_023": "Optimize PySpark code for shuffle-heavy workloads.",
    "pyspark_024": "Demonstrate repartition vs coalesce with examples.",
    "pyspark_025": "Apply partitioning and bucketing strategies for performance.",
    "pyspark_026": "Vectorize numerical columns for ML model training in PySpark.",
    "pyspark_027": "Perform dataset train-test split using randomSplit.",
    "pyspark_028": "Build a PySpark ML pipeline for RandomForestClassifier.",
    "pyspark_029": "Create a Python UDF to mask PII data.",
    "pyspark_030": "Write a Pandas UDF for rolling window features.",
    "pyspark_031": "Generate PySpark code for pivoting datasets.",
    "pyspark_032": "Explode arrays + structs and normalize them.",
    "pyspark_033": "Detect duplicates using window functions.",
    "pyspark_034": "Hash and anonymize sensitive fields.",
    "pyspark_035": "Compute business KPIs using chained transformations.",
    "pyspark_036": "Generate surrogate keys using monotonically_increasing_id().",
    "pyspark_037": "Join multiple DataFrames dynamically using a list of tables.",
    "pyspark_038": "Validate input schema matches expected schema.",
    "pyspark_039": "Implement audit logging for PySpark ETL jobs.",
    "pyspark_040": "Generate summary statistics for DataFrame profiling.",
    "pyspark_041": "Write reusable ETL functions in PySpark.",
    "pyspark_042": "Use flatMap and map for custom transformations.",
    "pyspark_043": "Implement complex conditional expressions using when/otherwise.",
    "pyspark_044": "Convert JSON strings into nested columns.",
    "pyspark_045": "Dynamically rename all columns using transformations.",
    "pyspark_046": "Read Parquet, ORC, Avro files from UC Volumes.",
    "pyspark_047": "Write streaming and batch DataFrame outputs to Delta.",
    "pyspark_048": "Implement sessionization logic for events.",
    "pyspark_049": "Apply sliding window aggregations.",
    "pyspark_050": "Generate PySpark unit tests using pytest.",
    "pyspark_051": "Compute KPIs per region and time window.",
    "pyspark_052": "Split DataFrame into N equal parts.",
    "pyspark_053": "Implement retry logic with exponential backoff.",
    "pyspark_054": "Detect null-heavy columns.",
    "pyspark_055": "Identify schema drift across batches.",
    "pyspark_056": "Safely convert PySpark DataFrame to Pandas.",
    "pyspark_057": "Create reusable ETL class structure.",
    "pyspark_058": "Generate metadata-driven PySpark code.",
    "pyspark_059": "Generate batch historical rebuild PySpark pipeline.",
    "pyspark_060": "Build a PySpark pipeline for Bronze → Silver → Gold transformations."
  },

  "SQL": {
    "sql_001": "SQL to create a Unity Catalog table with schema and constraints.",
    "sql_002": "Insert multiple rows into a Delta table using VALUES.",
    "sql_003": "Create an external UC table using a UC Volume.",
    "sql_004": "Select only columns containing a substring.",
    "sql_005": "Clean column names into snake_case.",
    "sql_006": "Replace all NULLs using COALESCE.",
    "sql_007": "Write SQL for inner, left, right, full joins.",
    "sql_008": "Write SQL to join 3+ tables with aliases.",
    "sql_009": "Perform SUM, AVG, MIN, MAX grouped by region, date.",
    "sql_010": "Conditional aggregations using CASE WHEN.",
    "sql_011": "SQL for rolling 7-day average using window functions.",
    "sql_012": "Rank customers by spend using RANK().",
    "sql_013": "MERGE INTO with update + insert logic.",
    "sql_014": "Perform time travel using VERSION AS OF.",
    "sql_015": "VACUUM Delta table older than N days.",
    "sql_016": "OPTIMIZE ZORDER BY best practices.",
    "sql_017": "Analyze table statistics for performance tuning.",
    "sql_018": "SQL pivot example for monthly sales.",
    "sql_019": "SQL unpivot example using stack().",
    "sql_020": "Explode JSON arrays using LATERAL VIEW.",
    "sql_021": "GRANT SELECT/USAGE on UC catalog objects.",
    "sql_022": "Column-level access control using UC policies.",
    "sql_023": "Row-level security using secure views.",
    "sql_024": "Calculate month-over-month growth.",
    "sql_025": "Build SCD Type 2 dimension table using MERGE.",
    "sql_026": "Generate fuzz matching SQL using SOUNDEX.",
    "sql_027": "Compare two tables and generate diff.",
    "sql_028": "Create metadata-driven SQL transformations.",
    "sql_029": "Generate SQL for cumulative KPIs.",
    "sql_030": "Produce daily aggregates for BI dashboards.",
    "sql_031": "Detect schema mismatch between two tables.",
    "sql_032": "Implement windowed time-series transformations.",
    "sql_033": "Create a secure materialized view.",
    "sql_034": "SQL for dynamic pivot across unknown categories.",
    "sql_035": "Use Regex extract in SQL to parse structured fields.",
    "sql_036": "Create SQL to detect anomalies in metrics.",
    "sql_037": "Generate SQL that validates data quality expectations.",
    "sql_038": "Create SQL script for deleting orphan records.",
    "sql_039": "Generate SQL for table lineage extraction.",
    "sql_040": "Convert SQL transformations into DLT pipeline logic.",
    "sql_041": "Build rolling retention cohort SQL.",
    "sql_042": "Union multiple tables dynamically.",
    "sql_043": "SQL to identify slow-moving products.",
    "sql_044": "Compute churn metrics via SQL.",
    "sql_045": "Create daily incremental load SQL template."
  },

  "DeltaOptimization": {
    "delta_001": "Inspect transaction logs in a Delta table.",
    "delta_002": "Generate OPTIMIZE ZORDER statements for performance.",
    "delta_003": "Show Delta table stats, partitions, files.",
    "delta_004": "Write compaction logic using OPTIMIZE + VACUUM.",
    "delta_005": "CDC MERGE pattern best practices.",
    "delta_006": "Merge streaming data with Delta efficiently.",
    "delta_007": "Implement schema evolution.",
    "delta_008": "Create Delta constraints to enforce data quality.",
    "delta_009": "Restore Delta table to earlier version.",
    "delta_010": "Optimize Delta for BI workloads.",
    "delta_011": "Generate Delta Change Data Feed (CDF) queries.",
    "delta_012": "Detect duplicate rows using Delta logs.",
    "delta_013": "Partition pruning optimization tips.",
    "delta_014": "Compact small files using auto-optimize.",
    "delta_015": "Write Delta retention policy strategy."
  },

  "UnityCatalog": {
    "uc_001": "Create catalog, schema, managed table in Unity Catalog.",
    "uc_002": "Create external UC Volume with storage location.",
    "uc_003": "Define Delta sharing provider in UC.",
    "uc_004": "Generate GRANT statements for secure access.",
    "uc_005": "Apply column-level access policies.",
    "uc_006": "Apply dynamic row filters in UC.",
    "uc_007": "Create secure UC functions.",
    "uc_008": "Model governance for MLflow models in UC.",
    "uc_009": "UC lineage for DE → ML → BI pipeline.",
    "uc_010": "Create UC tags and attach metadata.",
    "uc_011": "Build multi-env UC architecture (dev/test/prod)."
  },

  "LakeflowDLT": {
    "dlt_001": "Create Bronze, Silver, Gold DLT pipeline.",
    "dlt_002": "Add expectations with constraints.",
    "dlt_003": "Implement CDC ingestion using DLT.",
    "dlt_004": "Auto Loader + DLT integration.",
    "dlt_005": "Implement schema evolution in DLT.",
    "dlt_006": "Write modularized DLT functions.",
    "dlt_007": "Trigger DLT via Workflow.",
    "dlt_008": "Build data quality reporting using DLT."
  },

  "AIAssistant": {
    "aiassist_001": "Optimize PySpark code for better performance.",
    "aiassist_002": "Explain this stack trace and show fix.",
    "aiassist_003": "Convert Python ETL into DLT pipeline.",
    "aiassist_004": "Optimize this SQL query for performance.",
    "aiassist_005": "Rewrite this SQL into a Lakehouse best-practice version."
  },

  "MLflow": {
    "mlflow_001": "Log MLflow experiment with params, metrics.",
    "mlflow_002": "Transition UC model registry stages.",
    "mlflow_003": "Log artifacts (plots, files).",
    "mlflow_004": "Apply MLflow Tracing for LLM calls.",
    "mlflow_005": "Evaluate model drift using metrics."
  },

  "FeatureStore": {
    "feat_001": "Create UC Feature Table with primary key.",
    "feat_002": "Write offline feature lookup code.",
    "feat_003": "Write online feature lookup code.",
    "feat_004": "Create point-in-time correct dataset.",
    "feat_005": "Validate online/offline consistency."
  },

  "MosaicAI": {
    "mosaic_001": "Create serverless model serving endpoint for LLM.",
    "mosaic_002": "Generate embedding API request.",
    "mosaic_003": "Optimize inference using batching.",
    "mosaic_004": "Enable quantization for faster inference.",
    "mosaic_005": "Create custom LLM fine-tuning pipeline."
  },

  "VectorSearchRAG": {
    "rag_001": "Create Vector Search index with Delta Sync.",
    "rag_002": "Generate embeddings using MosaicAI models.",
    "rag_003": "Write hybrid search (vector + metadata).",
    "rag_004": "Build RAG retrieval pipeline.",
    "rag_005": "Implement reranking with cross-encoder.",
    "rag_006": "Evaluate RAG effectiveness using metrics.",
    "rag_007": "Build production-grade RAG chatbot."
  },

  "Agents": {
    "agent_001": "Create SQL-enabled AI agent using UC.",
    "agent_002": "Create ReAct prompt for stepwise reasoning.",
    "agent_003": "Add tool calling to agent (SQL + Python).",
    "agent_004": "Build multi-agent pipeline.",
    "agent_005": "Add safety guardrails to an AI agent."
  }
}
